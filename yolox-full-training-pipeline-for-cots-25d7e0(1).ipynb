{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Help Protect the Great Barrier Reefの[リファレンスノート](https://www.kaggle.com/remekkinas/yolox-training-pipeline-cots-dataset-lb-0-507)を日本語で解説します。オブジェクト検出は興味深いトピックであり、最先端の技術を使った[このノート](https://www.kaggle.com/remekkinas/yolox-training-pipeline-cots-dataset-lb-0-507)もとても興味深いのですが、初学者には理解の難しいものと思います。ここでは背景等の説明も加えて、これを読むだけで一定の理解に達することを目的とします。\n\n参考になったら投票お願いします！","metadata":{}},{"cell_type":"markdown","source":"# Train YOLOX on COTS dataset (PART 1 - TRAINING)\n\nこのノートブックは、Kaggleでカスタムオブジェクト検出モデル（COTS(ヒトデ)データセット）をトレーニングする方法を示しています。 YOLOX検出器に基づいて独自のカスタムモデルを構築するための良い出発点になる可能性があります。\n\nYOLO（You Look Only Onse “You Only Live Once“をもじったもの）の日本語説明は[ここ](https://www.renom.jp/ja/notebooks/tutorial/image_processing/yolo/notebook.html)が良いと思います。ここで見つけることができる完全なgithubリポジトリ - [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)\n\n<div align = 'center'><img src='https://github.com/Megvii-BaseDetection/YOLOX/raw/main/assets/logo.png'/></div>\n\n**このノートブックでカバーされている手順：**\n* Install YOLOX \n* Prepare COTS dataset for YOLOX object detection training\n* Download Pre-Trained Weights for YOLOX\n* Prepare configuration files\n* YOLOX training\n* Run YOLOX inference on test images\n* Export YOLOX weights for Tensorflow inference (soon)\n\n今、私はYOLOXで学習とプロトタイピングのためのノートブックを作成しました。 次のステップは、より良いモデルを作成することです（YOLOX実験パラメーターで遊んでください）。","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:34:31.033138Z","iopub.execute_input":"2021-11-29T13:34:31.033449Z","iopub.status.idle":"2021-11-29T13:34:33.455468Z","shell.execute_reply.started":"2021-11-29T13:34:31.033368Z","shell.execute_reply":"2021-11-29T13:34:33.454141Z"}}},{"cell_type":"markdown","source":"**キーポイント**\n* 提供されている **python time-series API**を使用して予測を送信する必要があります。これにより、このコンテストは以前のオブジェクト検出コンテストとは異なります。\n* 各予測行には、画像のすべての境界ボックスを含める必要があります。 提出はフォーマットも**COCO**のようです。これは `[x_min, y_min, width, height]`　です。\n* コンペメトリック `F2`は、ヒトデの見逃しを最小限に抑えるために、いくつかの誤検知（FP）を許容します。 つまり、**偽陰性（FN）**に取り組むことは、偽陽性（FP）よりも重要です。\n$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-success\" role=\"alert\">\nこの作業は2つの部分で構成されています。  \n    <ul>\n        <li> PART 1 - トレーニングカスタムモデル（COTSデータセット用）-> COTSデータセット用のYoloXフルトレーニングパイプライン->このノートブック</li>\n        <li> PART 2 - COTS用のKaggleのYOLOXが利用可能です-> <a href=\"https://www.kaggle.com/remekkinas/yolox-inference-on-kaggle-for-cots\">COTSデータセットで行われたYOLOX検出の提出 (PART 2 - 検出)</a></li>\n    </ul>\n    \n</div>","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport ast\nimport os\nimport json\nimport pandas as pd\nimport torch\nimport importlib\nimport cv2 \n\nfrom shutil import copyfile\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nfrom sklearn.model_selection import GroupKFold\nfrom PIL import Image\nfrom string import Template\nfrom IPython.display import display\n\nTRAIN_PATH = '/kaggle/input/tensorflow-great-barrier-reef'","metadata":{"execution":{"iopub.status.busy":"2021-11-30T20:30:28.750769Z","iopub.execute_input":"2021-11-30T20:30:28.751231Z","iopub.status.idle":"2021-11-30T20:30:31.071567Z","shell.execute_reply.started":"2021-11-30T20:30:28.751184Z","shell.execute_reply":"2021-11-30T20:30:31.070838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check Torch and CUDA version\nprint(f\"Torch: {torch.__version__}\")\n!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:04:52.407998Z","iopub.execute_input":"2021-11-30T16:04:52.408196Z","iopub.status.idle":"2021-11-30T16:04:53.102039Z","shell.execute_reply.started":"2021-11-30T16:04:52.408171Z","shell.execute_reply":"2021-11-30T16:04:53.10119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. INSTALL YOLOX","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/Megvii-BaseDetection/YOLOX -q\n\n%cd YOLOX\n!pip install -U pip && pip install -r requirements.txt\n!pip install -v -e . ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-30T16:04:53.103628Z","iopub.execute_input":"2021-11-30T16:04:53.103914Z","iopub.status.idle":"2021-11-30T16:05:51.064672Z","shell.execute_reply.started":"2021-11-30T16:04:53.103871Z","shell.execute_reply":"2021-11-30T16:05:51.063823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-30T16:05:51.067529Z","iopub.execute_input":"2021-11-30T16:05:51.067816Z","iopub.status.idle":"2021-11-30T16:06:07.407457Z","shell.execute_reply.started":"2021-11-30T16:05:51.067774Z","shell.execute_reply":"2021-11-30T16:06:07.406578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. PREPARE COTS DATASET FOR YOLOX\nこのセクションは、Awsafによって作成されたノートブックから抜粋したものです。 [Great-Barrier-Reef: YOLOv5 train](https://www.kaggle.com/awsaf49/great-barrier-reef-yolov5-train)\n\n## A. PREPARE DATASET AND ANNOTATIONS\n* このノートブックでは、 **bboxed-images** (`~5k`)のみを使用します。 電車には約23Kの画像をすべて使用できますが、ほとんどの画像にはラベルがありません。 したがって、**bboxed images**のみを使用して実験を実行する方が簡単です。","metadata":{}},{"cell_type":"code","source":"def get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_path(row):\n    row['image_path'] = f'{TRAIN_PATH}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n    return row","metadata":{"execution":{"iopub.status.busy":"2021-11-30T20:30:36.703739Z","iopub.execute_input":"2021-11-30T20:30:36.704451Z","iopub.status.idle":"2021-11-30T20:30:36.709101Z","shell.execute_reply.started":"2021-11-30T20:30:36.704412Z","shell.execute_reply":"2021-11-30T20:30:36.708332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/tensorflow-great-barrier-reef/train.csv\")\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T20:30:38.341409Z","iopub.execute_input":"2021-11-30T20:30:38.341939Z","iopub.status.idle":"2021-11-30T20:30:38.407828Z","shell.execute_reply.started":"2021-11-30T20:30:38.341899Z","shell.execute_reply":"2021-11-30T20:30:38.407018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taken only annotated photos\ndf[\"num_bbox\"] = df['annotations'].apply(lambda x: str.count(x, 'x'))\ndf_train = df[df[\"num_bbox\"]>0]\n\n#Annotations ast.literal_eval 文字列をPythonのリテラルとして見て、リストや辞書に変換する\ndf_train['annotations'] = df_train['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf_train['bboxes'] = df_train.annotations.progress_apply(get_bbox)\n\n#Images resolution\ndf_train[\"width\"] = 1280\ndf_train[\"height\"] = 720\n\n#Path of images\ndf_train = df_train.progress_apply(get_path, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T20:30:40.699233Z","iopub.execute_input":"2021-11-30T20:30:40.699944Z","iopub.status.idle":"2021-11-30T20:30:44.270794Z","shell.execute_reply.started":"2021-11-30T20:30:40.699897Z","shell.execute_reply":"2021-11-30T20:30:44.269949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = GroupKFold(n_splits = 5) \ndf_train = df_train.reset_index(drop=True)\ndf_train['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y = df_train.video_id.tolist(), groups=df_train.sequence)):\n    df_train.loc[val_idx, 'fold'] = fold\n\ndf_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T20:30:46.41906Z","iopub.execute_input":"2021-11-30T20:30:46.419583Z","iopub.status.idle":"2021-11-30T20:30:46.44959Z","shell.execute_reply.started":"2021-11-30T20:30:46.419544Z","shell.execute_reply":"2021-11-30T20:30:46.44894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**画像書き込み**\n* `/kaggle/input`には**YOLOv5**に必要な書き込みアクセス権がないため、イメージを現在のディレクトリ(`/kaggle/working`)にコピーする必要があります。\n* **並列計算**を使用する**Joblib**を使用すると、このプロセスを高速化できます。","metadata":{}},{"cell_type":"code","source":"HOME_DIR = '/kaggle/working/' \nDATASET_PATH = 'dataset/images'\n\n!mkdir {HOME_DIR}dataset\n!mkdir {HOME_DIR}{DATASET_PATH}\n!mkdir {HOME_DIR}{DATASET_PATH}/train2017\n!mkdir {HOME_DIR}{DATASET_PATH}/val2017\n!mkdir {HOME_DIR}{DATASET_PATH}/annotations","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:06:11.317733Z","iopub.execute_input":"2021-11-30T16:06:11.31814Z","iopub.status.idle":"2021-11-30T16:06:14.833355Z","shell.execute_reply.started":"2021-11-30T16:06:11.318103Z","shell.execute_reply":"2021-11-30T16:06:14.832169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SELECTED_FOLD = 4\n\nfor i in tqdm(range(len(df_train))): #「進捗状況や処理状況をプログレスバー（ステータスバー）として表示」する機能\n    row = df_train.loc[i]\n    if row.fold != SELECTED_FOLD:\n        copyfile(f'{row.image_path}', f'{HOME_DIR}{DATASET_PATH}/train2017/{row.image_id}.jpg')\n    else:\n        copyfile(f'{row.image_path}', f'{HOME_DIR}{DATASET_PATH}/val2017/{row.image_id}.jpg') ","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:06:14.837842Z","iopub.execute_input":"2021-11-30T16:06:14.838096Z","iopub.status.idle":"2021-11-30T16:07:20.413218Z","shell.execute_reply.started":"2021-11-30T16:06:14.838064Z","shell.execute_reply":"2021-11-30T16:07:20.412231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of training files: {len(os.listdir(f\"{HOME_DIR}{DATASET_PATH}/train2017/\"))}')\nprint(f'Number of validation files: {len(os.listdir(f\"{HOME_DIR}{DATASET_PATH}/val2017/\"))}')","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:07:20.416041Z","iopub.execute_input":"2021-11-30T16:07:20.416247Z","iopub.status.idle":"2021-11-30T16:07:20.425391Z","shell.execute_reply.started":"2021-11-30T16:07:20.416221Z","shell.execute_reply":"2021-11-30T16:07:20.424535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B. CREATE COCO ANNOTATION FILES\n\n**ラベル作成**\n\nラベルを**YOLO**形式にエクスポートする必要があります。画像毎に1つの`*.txt`ファイルを使用します（画像にオブジェクトがない場合は`*.txt`ファイルは必要ありません）。仕様は次の通りです。\n\n* オブジェクトごとに1行\n* 各行は`[x_center, y_center, width, height]`形式\n* Boxは**正規化**された`xywh`形式 (`0 - 1`)である必要があります。 Boxesがピクセル単位の場合は、`x_center`と`width`を`image width`で割り、`y_center`と`height`を`image height`で割ります。\n* クラス番号は**zero-indexed**です（0から始まります）。\n\n> コンペのbboxは**COCO**形式であるため`[x_min, y_min, width, height]`です。したがって、**COCO**形式を**YOLO** 形式に変換する必要があります。\n","metadata":{}},{"cell_type":"code","source":"def save_annot_json(json_annotation, filename):\n    with open(filename, 'w') as f:\n        output_json = json.dumps(json_annotation)\n        f.write(output_json)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:07:20.42675Z","iopub.execute_input":"2021-11-30T16:07:20.42733Z","iopub.status.idle":"2021-11-30T16:07:20.433484Z","shell.execute_reply.started":"2021-11-30T16:07:20.427277Z","shell.execute_reply":"2021-11-30T16:07:20.432717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotion_id = 0","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:07:20.434766Z","iopub.execute_input":"2021-11-30T16:07:20.435123Z","iopub.status.idle":"2021-11-30T16:07:20.442743Z","shell.execute_reply.started":"2021-11-30T16:07:20.435087Z","shell.execute_reply":"2021-11-30T16:07:20.441883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataset2coco(df, dest_path):\n    \n    global annotion_id\n    \n    annotations_json = {\n        \"info\": [],\n        \"licenses\": [],\n        \"categories\": [],\n        \"images\": [],\n        \"annotations\": []\n    }\n    \n    info = {\n        \"year\": \"2021\",\n        \"version\": \"1\",\n        \"description\": \"COTS dataset - COCO format\",\n        \"contributor\": \"\",\n        \"url\": \"https://kaggle.com\",\n        \"date_created\": \"2021-11-30T15:01:26+00:00\"\n    }\n    annotations_json[\"info\"].append(info)\n    \n    lic = {\n            \"id\": 1,\n            \"url\": \"\",\n            \"name\": \"Unknown\"\n        }\n    annotations_json[\"licenses\"].append(lic)\n\n    classes = {\"id\": 0, \"name\": \"starfish\", \"supercategory\": \"none\"}\n\n    annotations_json[\"categories\"].append(classes)\n\n    \n    for ann_row in df.itertuples():\n            \n        images = {\n            \"id\": ann_row[0],\n            \"license\": 1,\n            \"file_name\": ann_row.image_id + '.jpg',\n            \"height\": ann_row.height,\n            \"width\": ann_row.width,\n            \"date_captured\": \"2021-11-30T15:01:26+00:00\"\n        }\n        \n        annotations_json[\"images\"].append(images)\n        \n        bbox_list = ann_row.bboxes\n        \n        for bbox in bbox_list:\n            b_width = bbox[2]\n            b_height = bbox[3]\n            \n            # some boxes in COTS are outside the image height and width\n            if (bbox[0] + bbox[2] > 1280):\n                b_width = bbox[0] - 1280 \n            if (bbox[1] + bbox[3] > 720):\n                b_height = bbox[1] - 720 \n                \n            image_annotations = {\n                \"id\": annotion_id,\n                \"image_id\": ann_row[0],\n                \"category_id\": 0,\n                \"bbox\": [bbox[0], bbox[1], b_width, b_height],\n                \"area\": bbox[2] * bbox[3],\n                \"segmentation\": [],\n                \"iscrowd\": 0\n            }\n            \n            annotion_id += 1\n            annotations_json[\"annotations\"].append(image_annotations)\n        \n        \n    print(f\"Dataset COTS annotation to COCO json format completed! Files: {len(df)}\")\n    return annotations_json","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:35:10.71171Z","iopub.execute_input":"2021-11-30T16:35:10.711976Z","iopub.status.idle":"2021-11-30T16:35:10.724881Z","shell.execute_reply.started":"2021-11-30T16:35:10.711945Z","shell.execute_reply":"2021-11-30T16:35:10.724187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert COTS dataset to JSON COCO\ntrain_annot_json = dataset2coco(df_train[df_train.fold != SELECTED_FOLD], f\"{HOME_DIR}{DATASET_PATH}/train2017/\")\nval_annot_json = dataset2coco(df_train[df_train.fold == SELECTED_FOLD], f\"{HOME_DIR}{DATASET_PATH}/val2017/\")\n\n# Save converted annotations\nsave_annot_json(train_annot_json, f\"{HOME_DIR}{DATASET_PATH}/annotations/train.json\")\nsave_annot_json(val_annot_json, f\"{HOME_DIR}{DATASET_PATH}/annotations/valid.json\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:35:15.110479Z","iopub.execute_input":"2021-11-30T16:35:15.111016Z","iopub.status.idle":"2021-11-30T16:35:15.221022Z","shell.execute_reply.started":"2021-11-30T16:35:15.110977Z","shell.execute_reply":"2021-11-30T16:35:15.220167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. PREPARE CONFIGURATION FILE\n\nYoloxnoの設定ファイル:\n- [YOLOX-nano](https://github.com/Megvii-BaseDetection/YOLOX/blob/main/exps/default/nano.py)\n- [YOLOX-s](https://github.com/Megvii-BaseDetection/YOLOX/blob/main/exps/default/yolox_s.py)\n- [YOLOX-m](https://github.com/Megvii-BaseDetection/YOLOX/blob/main/exps/default/yolox_m.py)\n\n以下に、COTSデータセットトレーニング用の2つの（yolox-sとyolox-nano）構成ファイルを示します。\n\n<div align=\"center\"><img  width=\"800\" src=\"https://github.com/Megvii-BaseDetection/YOLOX/raw/main/assets/git_fig.png\"/></div>","metadata":{}},{"cell_type":"code","source":"# Choose model for your experiments NANO or YOLOX-S (you can adapt for other model type)\n\nNANO = False","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:23:24.134777Z","iopub.execute_input":"2021-11-30T16:23:24.135542Z","iopub.status.idle":"2021-11-30T16:23:24.140573Z","shell.execute_reply.started":"2021-11-30T16:23:24.135506Z","shell.execute_reply":"2021-11-30T16:23:24.139599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3A. YOLOX-S EXPERIMENT CONFIGURATION FILE\nトレーニングパラメータは、実験設定ファイルで設定できます。 YOLOX-sとnanoのカスタムファイルを作成しました。 元のgithubリポジトリのファイルを使用して独自のファイルを作成できます。","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n<strong> For YOLOX_s I use input size 960x960 but you can change it for your experiments.</strong> \n</div>","metadata":{}},{"cell_type":"code","source":"config_file_template = '''\n\n#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Copyright (c) Megvii, Inc. and its affiliates.\n\nimport os\n\nfrom yolox.exp import Exp as MyExp\n\n\nclass Exp(MyExp):\n    def __init__(self):\n        super(Exp, self).__init__()\n        self.depth = 0.33\n        self.width = 0.50\n        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\".\")[0]\n        \n        # Define yourself dataset path\n        self.data_dir = \"/kaggle/working/dataset/images\"\n        self.train_ann = \"train.json\"\n        self.val_ann = \"valid.json\"\n\n        self.num_classes = 1\n\n        self.max_epoch = $max_epoch\n        self.data_num_workers = 2\n        self.eval_interval = 1\n        \n        self.mosaic_prob = 1.0\n        self.mixup_prob = 1.0\n        self.hsv_prob = 1.0\n        self.flip_prob = 0.5\n        self.no_aug_epochs = 2\n        \n        self.input_size = (960, 960)\n        self.mosaic_scale = (0.5, 1.5)\n        self.random_size = (10, 20)\n        self.test_size = (960, 960)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:58:48.093892Z","iopub.execute_input":"2021-11-30T16:58:48.094792Z","iopub.status.idle":"2021-11-30T16:58:48.100179Z","shell.execute_reply.started":"2021-11-30T16:58:48.094747Z","shell.execute_reply":"2021-11-30T16:58:48.099216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3B. YOLOX-NANO CONFIG FILE\n<div class=\"alert alert-warning\">\n<strong> For YOLOX_nano I use input size 460x460 but you can change it for your experiments.</strong> \n</div","metadata":{}},{"cell_type":"code","source":"if NANO:\n    config_file_template = '''\n\n#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Copyright (c) Megvii, Inc. and its affiliates.\n\nimport os\n\nimport torch.nn as nn\n\nfrom yolox.exp import Exp as MyExp\n\n\nclass Exp(MyExp):\n    def __init__(self):\n        super(Exp, self).__init__()\n        self.depth = 0.33\n        self.width = 0.25\n        self.input_size = (416, 416)\n        self.mosaic_scale = (0.5, 1.5)\n        self.random_size = (10, 20)\n        self.test_size = (416, 416)\n        self.exp_name = os.path.split(\n            os.path.realpath(__file__))[1].split(\".\")[0]\n        self.enable_mixup = False\n\n        # Define yourself dataset path\n        self.data_dir = \"/kaggle/working/dataset/images\"\n        self.train_ann = \"train.json\"\n        self.val_ann = \"valid.json\"\n\n        self.num_classes = 1\n\n        self.max_epoch = $max_epoch\n        self.data_num_workers = 2\n        self.eval_interval = 1\n\n    def get_model(self, sublinear=False):\n        def init_yolo(M):\n            for m in M.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eps = 1e-3\n                    m.momentum = 0.03\n\n        if \"model\" not in self.__dict__:\n            from yolox.models import YOLOX, YOLOPAFPN, YOLOXHead\n            in_channels = [256, 512, 1024]\n            # NANO model use depthwise = True, which is main difference.\n            backbone = YOLOPAFPN(self.depth,\n                                 self.width,\n                                 in_channels=in_channels,\n                                 depthwise=True)\n            head = YOLOXHead(self.num_classes,\n                             self.width,\n                             in_channels=in_channels,\n                             depthwise=True)\n            self.model = YOLOX(backbone, head)\n\n        self.model.apply(init_yolo)\n        self.model.head.initialize_biases(1e-2)\n        return self.model\n\n'''","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:18:13.560848Z","iopub.execute_input":"2021-11-30T16:18:13.561771Z","iopub.status.idle":"2021-11-30T16:18:13.567603Z","shell.execute_reply.started":"2021-11-30T16:18:13.561722Z","shell.execute_reply":"2021-11-30T16:18:13.56663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n<strong> モデルを5EPOCHSのみでトレーニングしました....これはデモ目的のみです。 </strong> \n</div>","metadata":{}},{"cell_type":"code","source":"PIPELINE_CONFIG_PATH='cots_config.py'\n\npipeline = Template(config_file_template).substitute(max_epoch = 5)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:58:53.179842Z","iopub.execute_input":"2021-11-30T16:58:53.180526Z","iopub.status.idle":"2021-11-30T16:58:53.185981Z","shell.execute_reply.started":"2021-11-30T16:58:53.180488Z","shell.execute_reply":"2021-11-30T16:58:53.185265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ./yolox/data/datasets/voc_classes.py\n\nvoc_cls = '''\nVOC_CLASSES = (\n  \"starfish\",\n)\n'''\nwith open('./yolox/data/datasets/voc_classes.py', 'w') as f:\n    f.write(voc_cls)\n\n# ./yolox/data/datasets/coco_classes.py\n\ncoco_cls = '''\nCOCO_CLASSES = (\n  \"starfish\",\n)\n'''\nwith open('./yolox/data/datasets/coco_classes.py', 'w') as f:\n    f.write(coco_cls)\n\n# check if everything is ok    \n!more ./yolox/data/datasets/coco_classes.py","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:35:30.283836Z","iopub.execute_input":"2021-11-30T16:35:30.284583Z","iopub.status.idle":"2021-11-30T16:35:30.965767Z","shell.execute_reply.started":"2021-11-30T16:35:30.284543Z","shell.execute_reply":"2021-11-30T16:35:30.964967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. DOWNLOAD PRETRAINED WEIGHTS","metadata":{}},{"cell_type":"markdown","source":"List of pretrained models:\n* YOLOX-s\n* YOLOX-m\n* YOLOX-nano for inference speed (!)\n* etc.","metadata":{}},{"cell_type":"code","source":"sh = 'wget https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth'\nMODEL_FILE = 'yolox_s.pth'\n\nif NANO:\n    sh = '''\n    wget https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_nano.pth\n    '''\n    MODEL_FILE = 'yolox_nano.pth'\n\nwith open('script.sh', 'w') as file:\n  file.write(sh)\n\n!bash script.sh","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:24:16.590401Z","iopub.execute_input":"2021-11-30T16:24:16.591075Z","iopub.status.idle":"2021-11-30T16:24:17.977995Z","shell.execute_reply.started":"2021-11-30T16:24:16.591028Z","shell.execute_reply":"2021-11-30T16:24:17.977091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. TRAIN MODEL","metadata":{}},{"cell_type":"code","source":"!cp ./tools/train.py ./","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:35:38.8627Z","iopub.execute_input":"2021-11-30T16:35:38.8636Z","iopub.status.idle":"2021-11-30T16:35:39.524183Z","shell.execute_reply.started":"2021-11-30T16:35:38.863551Z","shell.execute_reply":"2021-11-30T16:35:39.52321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python train.py \\\n    -f cots_config.py \\\n    -d 1 \\\n    -b 32 \\\n    --fp16 \\\n    -o \\\n    -c {MODEL_FILE}   # Remember to chenge this line if you take different model eg. yolo_nano.pth, yolox_s.pth or yolox_m.pth","metadata":{"execution":{"iopub.status.busy":"2021-11-30T16:58:59.338136Z","iopub.execute_input":"2021-11-30T16:58:59.338524Z","iopub.status.idle":"2021-11-30T17:38:17.948724Z","shell.execute_reply.started":"2021-11-30T16:58:59.338486Z","shell.execute_reply":"2021-11-30T17:38:17.947826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. RUN INFERENCE\n\n## 6A. INFERENCE USING YOLOX TOOL","metadata":{}},{"cell_type":"code","source":"# I have to fix demo.py file because it:\n# - raises error in Kaggle (cvWaitKey does not work) \n# - saves result files in time named directory eg. /2021_11_29_22_51_08/ which is difficult then to automatically show results\n\n%cp ../../input/yolox-kaggle-fix-for-demo-inference/demo.py tools/demo.py","metadata":{"execution":{"iopub.status.busy":"2021-11-30T17:40:23.529552Z","iopub.execute_input":"2021-11-30T17:40:23.530352Z","iopub.status.idle":"2021-11-30T17:40:24.228418Z","shell.execute_reply.started":"2021-11-30T17:40:23.530309Z","shell.execute_reply":"2021-11-30T17:40:24.227357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_IMAGE_PATH = \"/kaggle/working/dataset/images/val2017/0-4614.jpg\"\nMODEL_PATH = \"./YOLOX_outputs/cots_config/best_ckpt.pth\"\n\n!python tools/demo.py image \\\n    -f cots_config.py \\\n    -c {MODEL_PATH} \\\n    --path {TEST_IMAGE_PATH} \\\n    --conf 0.1 \\\n    --nms 0.45 \\\n    --tsize 960 \\\n    --save_result \\\n    --device gpu","metadata":{"execution":{"iopub.status.busy":"2021-11-30T17:40:25.954466Z","iopub.execute_input":"2021-11-30T17:40:25.955472Z","iopub.status.idle":"2021-11-30T17:40:31.903796Z","shell.execute_reply.started":"2021-11-30T17:40:25.955415Z","shell.execute_reply":"2021-11-30T17:40:31.90292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_IMAGE_PATH = \"./YOLOX_outputs/cots_config/vis_res/0-4614.jpg\" \nImage.open(OUTPUT_IMAGE_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T17:40:33.958824Z","iopub.execute_input":"2021-11-30T17:40:33.959161Z","iopub.status.idle":"2021-11-30T17:40:34.301375Z","shell.execute_reply.started":"2021-11-30T17:40:33.95912Z","shell.execute_reply":"2021-11-30T17:40:34.299544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6B. INFERENCE USING CUSTOM SCRIPT (IT WOULD BE USED FOR COTS INFERENCE PART)\n\n### 6B.1 SETUP MODEL","metadata":{}},{"cell_type":"code","source":"from yolox.utils import postprocess\nfrom yolox.data.data_augment import ValTransform\n\nCOCO_CLASSES = (\n  \"starfish\",\n)\n\n# get YOLOX experiment\ncurrent_exp = importlib.import_module('cots_config')\nexp = current_exp.Exp()\n\n# set inference parameters\ntest_size = (960, 960)\nnum_classes = 1\nconfthre = 0.1\nnmsthre = 0.45\n\n\n# get YOLOX model\nmodel = exp.get_model()\nmodel.cuda()\nmodel.eval()\n\n# get custom trained checkpoint\nckpt_file = \"./YOLOX_outputs/cots_config/best_ckpt.pth\"\nckpt = torch.load(ckpt_file, map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-30T17:40:47.581962Z","iopub.execute_input":"2021-11-30T17:40:47.582465Z","iopub.status.idle":"2021-11-30T17:40:49.981668Z","shell.execute_reply.started":"2021-11-30T17:40:47.582427Z","shell.execute_reply":"2021-11-30T17:40:49.980977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6B.2 INFERENCE BBOXES","metadata":{}},{"cell_type":"code","source":"def yolox_inference(img, model, test_size): \n    bboxes = []\n    bbclasses = []\n    scores = []\n    \n    preproc = ValTransform(legacy = False)\n\n    tensor_img, _ = preproc(img, None, test_size)\n    tensor_img = torch.from_numpy(tensor_img).unsqueeze(0)\n    tensor_img = tensor_img.float()\n    tensor_img = tensor_img.cuda()\n\n    with torch.no_grad():\n        outputs = model(tensor_img)\n        outputs = postprocess(\n                    outputs, num_classes, confthre,\n                    nmsthre, class_agnostic=True\n                )\n\n    if outputs[0] is None:\n        return [], [], []\n    \n    outputs = outputs[0].cpu()\n    bboxes = outputs[:, 0:4]\n\n    bboxes /= min(test_size[0] / img.shape[0], test_size[1] / img.shape[1])\n    bbclasses = outputs[:, 6]\n    scores = outputs[:, 4] * outputs[:, 5]\n    \n    return bboxes, bbclasses, scores","metadata":{"execution":{"iopub.status.busy":"2021-11-30T17:44:42.016709Z","iopub.execute_input":"2021-11-30T17:44:42.017298Z","iopub.status.idle":"2021-11-30T17:44:42.024464Z","shell.execute_reply.started":"2021-11-30T17:44:42.017256Z","shell.execute_reply":"2021-11-30T17:44:42.023731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6B.3 DRAW RESULT","metadata":{}},{"cell_type":"code","source":"def draw_yolox_predictions(img, bboxes, scores, bbclasses, confthre, classes_dict):\n    for i in range(len(bboxes)):\n            box = bboxes[i]\n            cls_id = int(bbclasses[i])\n            score = scores[i]\n            if score < confthre:\n                continue\n            x0 = int(box[0])\n            y0 = int(box[1])\n            x1 = int(box[2])\n            y1 = int(box[3])\n\n            cv2.rectangle(img, (x0, y0), (x1, y1), (0, 255, 0), 2)\n            cv2.putText(img, '{}:{:.1f}%'.format(classes_dict[cls_id], score * 100), (x0, y0 - 3), cv2.FONT_HERSHEY_PLAIN, 0.8, (0,255,0), thickness = 1)\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-11-30T17:40:55.799936Z","iopub.execute_input":"2021-11-30T17:40:55.80021Z","iopub.status.idle":"2021-11-30T17:40:55.80991Z","shell.execute_reply.started":"2021-11-30T17:40:55.800179Z","shell.execute_reply":"2021-11-30T17:40:55.807375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6B.4 ALL PUZZLES TOGETHER","metadata":{}},{"cell_type":"code","source":"TEST_IMAGE_PATH = \"/kaggle/working/dataset/images/val2017/0-4614.jpg\"\nimg = cv2.imread(TEST_IMAGE_PATH)\n\n# Get predictions\nbboxes, bbclasses, scores = yolox_inference(img, model, test_size)\n\n# Draw predictions\nout_image = draw_yolox_predictions(img, bboxes, scores, bbclasses, confthre, COCO_CLASSES)\n\n# Since we load image using OpenCV we have to convert it \nout_image = cv2.cvtColor(out_image, cv2.COLOR_BGR2RGB)\ndisplay(Image.fromarray(out_image))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T17:44:46.826158Z","iopub.execute_input":"2021-11-30T17:44:46.826988Z","iopub.status.idle":"2021-11-30T17:44:47.169804Z","shell.execute_reply.started":"2021-11-30T17:44:46.826943Z","shell.execute_reply":"2021-11-30T17:44:47.169166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-success\" role=\"alert\">\n    Find this notebook helpful? :) Please give me a vote ;) Thank you\n </div>","metadata":{}},{"cell_type":"markdown","source":"# 7. SUBMIT TO COTS COMPETITION AND EVALUATE","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\n\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()  ","metadata":{"execution":{"iopub.status.busy":"2021-11-30T17:41:53.604564Z","iopub.execute_input":"2021-11-30T17:41:53.605106Z","iopub.status.idle":"2021-11-30T17:41:53.631616Z","shell.execute_reply.started":"2021-11-30T17:41:53.605068Z","shell.execute_reply":"2021-11-30T17:41:53.630689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor (image_np, sample_prediction_df) in iter_test:\n \n    bboxes, bbclasses, scores = yolox_inference(image_np, model, test_size)\n    \n    predictions = []\n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        cls_id = int(bbclasses[i])\n        score = scores[i]\n        if score < confthre:\n            continue\n        x_min = int(box[0])\n        y_min = int(box[1])\n        x_max = int(box[2])\n        y_max = int(box[3])\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n\n    print('Prediction:', prediction_str)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T17:44:53.457988Z","iopub.execute_input":"2021-11-30T17:44:53.458918Z","iopub.status.idle":"2021-11-30T17:44:53.467057Z","shell.execute_reply.started":"2021-11-30T17:44:53.458871Z","shell.execute_reply":"2021-11-30T17:44:53.466236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T17:46:07.26512Z","iopub.execute_input":"2021-11-30T17:46:07.2654Z","iopub.status.idle":"2021-11-30T17:46:07.279331Z","shell.execute_reply.started":"2021-11-30T17:46:07.265368Z","shell.execute_reply":"2021-11-30T17:46:07.27862Z"},"trusted":true},"execution_count":null,"outputs":[]}]}